#!/usr/bin/env ruby

# fetch stories based on an 'accepted' date range
# summarize number of stories and number of points per label
# separate epic labels from non-epic labels

require 'net/http'
require 'json'
require 'optparse'
require 'set'
require 'date'
require 'time'
# require 'byebug'

project_id = ENV['PIVOTAL_TRACKER_PROJECT_ID']
token = ENV['PIVOTAL_TRACKER_API_TOKEN']

sortable_columns = ['name', 'points', 'features', 'bugs', 'chores']

options = { sort: 'name' }
OptionParser.new do |parser|
  parser.accept(Time) do |input|
    Time.parse(input)
  end

  parser.on("-w", "--weeks NUM", Integer, "Number of weeks to report on.") do |o|
    options[:week_count] = o
  end

  parser.on("-b", "--begin [DATE]", Time, "Date to begin.") do |o|
    options[:begin_at] = o
  end

  parser.on("-e", "--end [DATE]", Time, "Date to end.") do |o|
    options[:end_at] = o
  end

  parser.on("-s", "--sort COLUMN", "Which column to sort by. Valid options: #{sortable_columns.join(', ')}") do |o|
    options[:sort] = o
  end

end.parse!

if !sortable_columns.include?(options[:sort])
  puts "Invalid sort value '#{options[:sort]}' given. Use one of: #{sortable_columns.join(', ')}"
  exit 1
end

if options[:begin_at] && options[:end_at]
  begin_at = options[:begin_at]
  end_at = options[:end_at]
else
  # figure out date range of most-recently-completed N weeks. (based on options.)
  week_count = options[:week_count] || 1

  d = Date.today
  previous_sunday = d - d.wday
  end_at = previous_sunday.to_time.utc
  begin_at = (previous_sunday - (7 * week_count)).to_time.utc
end

def get_json(url, token)
  uri = URI.parse(url)

  req = Net::HTTP::Get.new(uri)
  req['X-TrackerToken'] = token
  begin
    response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: true) do |http|
      http.request(req)
    end
  rescue => e
    puts "Error communicating with Pivotal Tracker API. #{e.message}"
    exit 1
  end

  JSON.parse(response.body)
end

# add a single story's data to the summary and totals hashes.
def add_data_item(item, summary, totals, epics, max_name_length)
  label_names = item['labels'].map{ |i| i['name']}
  skip = (label_names & %w[duplicate canceled cancelled]).size > 0
  return max_name_length if skip

  item['labels'].each do |label|
    parts = label['name'].split(':')
    epic = epics[label['id']]
    if epic
      type = 'epic'
      name = epic['name']
    # group labels like 'app:foo' or 'system:bar'
    elsif parts.size > 1
      type = parts[0].strip
      name = parts[1..-1].join(':').strip
    else
      type = 'label'
      name = label['name']
    end

    summary[label['id']] ||= {
      'name' => '',
      'features' => 0,
      'bugs' => 0,
      'chores' => 0,
      'points' => 0,
      'type' => type
    }
    summary_item = summary[label['id']]
    summary_item['name'] = name
    case item['story_type']
    when 'feature'
      then summary_item['features'] += 1
    when 'bug'
      then summary_item['bugs'] += 1
    when 'chore'
      then summary_item['chores'] += 1
    end

    summary_item['points'] += item['estimate'].to_i
    max_name_length = [max_name_length, name.size].max
  end

  case item['story_type']
  when 'feature'
    then totals['features'] += 1
  when 'bug'
    then totals['bugs'] += 1
  when 'chore'
    then totals['chores'] += 1
  end

  totals['points'] += item['estimate'].to_i

  max_name_length
end

# access a PT api url and yield the found data items.
#
# navigates across paged data sets and yields all items across all pages.
def consume_paginated_url(base_url, token, items_per_page: 100)
  limit = items_per_page
  offset = 0
  loop do
    paginated_url = "#{base_url}&limit=#{limit}&offset=#{offset}&envelope=true"

    data = get_json(paginated_url, token)
    data['data'].each do |item|
      yield item
    end

    if data['pagination'] && (data['pagination']['returned'] == data['pagination']['limit'])
      offset += limit
    else
      break
    end
  end
end

# fetch epics, build set of label ids which are epics
# https://www.pivotaltracker.com/services/v5/projects/$PROJECT_ID/epics
#
# response is not paginated, and adding pagination params raises an error.
# https://www.pivotaltracker.com/help/api/rest/v5#Epics
# epic_ids = Set.new
url = "https://www.pivotaltracker.com/services/v5/projects/#{project_id}/epics"
epics = {}
get_json(url, token).each do |item|
  epics[item['label']['id']] = item
end

# key: label id
# value: hash. keys name, features, chores, bugs, points
summary = {}
totals = {'name' => 'TOTALS', 'features' => 0, 'bugs' => 0, 'chores' => 0, 'points' => 0}
max_name_length = 0

url = "https://www.pivotaltracker.com/services/v5/projects/#{project_id}/stories?accepted_after=#{begin_at.iso8601}&accepted_before=#{end_at.iso8601}"
consume_paginated_url(url, token) do |item|
  max_name_length = add_data_item(item, summary, totals, epics, max_name_length)
end

# sort by is_epic, then by user-specified column.
# print new header row after is_epic changes
summary = summary
  .sort_by { |id, item| "#{item['type']} #{item[options[:sort]].to_s}" }
  .to_h

puts "#{begin_at.to_date} - #{end_at.to_date}"

def header_row(max_name_length)
  ' ' * max_name_length + " points features bugs chores"
end

def data_row(item, max_name_length)
  "#{item['name'].ljust(max_name_length)} #{item['points'].to_s.rjust(6)} #{item['features'].to_s.rjust(8)} #{item['bugs'].to_s.rjust(4)} #{item['chores'].to_s.rjust(6)}"
end

prev_type = nil
summary.each do |id, item|
  current_type = item['type']
  if current_type != prev_type
    puts
    puts current_type.upcase + 'S'
    puts header_row(max_name_length)
  end
  puts data_row(item, max_name_length)
  prev_type = current_type
end

puts
# puts 'TOTALS'
puts header_row(max_name_length)
puts data_row(totals, max_name_length)
